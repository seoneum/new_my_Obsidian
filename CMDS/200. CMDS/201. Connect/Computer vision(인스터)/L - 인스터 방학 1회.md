---
type: lecture
title: L - 인스터 방학 1회
created: 2025-12-28
updated: 2025-12-28T21:04:35
author: "[[김선음]]"
group: SE
status: "[[🚜In Progress]]"
tags:
  - lecture
  - lecture/SE
  - tagging/needed
aliases: []
course: 인스터 Computer vision 방학 1
session: 2025-12-28
---

# L - 인스터 방학 1회

## Meta
- Course: 인스터 Computer vision 방학 1
- Session: 2025-12-28
- Instructor: 
- URL: 

## Outline
- 

## Raw Notes
- 전 시간 복기
	- 디텍션, 컨볼루션, 가우시안, 소벨 커널, log, dog, 
- Feature
	- 얼굴은 얼굴, 몸은 몸 이런식으로 구분하는 것
	- 여러가지 레벨 존재함. 
	- high, middle, low
	- high : 인간 수준 맥락적, 의미적 이해
	- middle : parches, regions 등 다리, 발톱 등등
	- low : pixel수준. edge, corner, texture.
	- ML, DL있음. 
	- ML : 처음에는 사람이 지도해줌
	- DL처음부터 끝까지 알아서 함.
	- Local Feature : 전체적 feature. 살짝 안좋음. 가려짐, 각도 다름, 종 내에 개체간 구분힘듦
	- Bag of words : Feature가져와서 patch수준으로 매치하는 것 같음
	- 같은 객체를 다른 각도, 시간, 그런 것들에서 구분을 해야겠지.
	- 그런 다른 조건에서 3d재구성 가능.
	- 같은 대상에서 변하지 않는 good descriptor 있어야함.
	- 개별적, 명확적, 압축/효율성 좋아야하고, 특징성 좋아야하고 시간, 색 변해도 구분 가능해야함.
	- Key-point - localization, 2,3
	- 하늘은 변화가 적어서 좃구림
	- edge는 색변화 있음 -> 그래디언트 있음.
	- corner : 하나의 alignment match. 방향성 좋음. 그래디언트 전 방향.
	- local feature를 위해 pixel이 아닌 patch, window수준에서 따는 것이 좋다.
	-  patch움직이고 차이를 빼서 제곱 때림. 
	- E(u,v)=x,y∑​w(x,y)\[I(x+u,y+v)−I(x,y)]2
	- 테일러 근사로 계산량 줄이기
	- 그냥 근사 근사 하면서 행렬로 족치기
	- f함수에 따라 peak가 다르다. 어쨋든 좋은건 피크 하나고 구분 잘되고 어쩌구 저쩌구
	- 그럼 어떤 f가 좋은 함수에요? LoG요~ 그냥 개 똥극적일듯.
	- LoG연산량 좃대네~ 어떡해!!! -> DoG쓰자잉~
	- 이게 sift여
	- 가우시안해서 존내 빼며는 LoG랑 비슷해~ 

## AI 정리

# Computer Vision: 6강 - 특징점 기술자 (Feature Descriptors)

## 1. 강의 소개 (Introduction)

이번 강의의 핵심 목표는 **"이미지 매칭(Image Matching)"** 입니다. 서로 다른 두 이미지(예: 파노라마 사진, 서로 다른 시점의 물체)에서 **동일한 지점(Correspondence)** 을 어떻게 컴퓨터가 찾아낼 수 있을까요?

이를 위해 우리는 이미지 전체가 아닌, **지역적 특징(Local Features)** 에 집중합니다.

> [!info] Feature Levels
> * **High-level**: 객체(Object), 장면(Scene) 등 의미론적 정보.
> * **Middle-level**: 영역(Region), 윤곽선(Contour).
> * **Low-level**: 엣지(Edge), 코너(Corner), 텍스처(Texture). $\rightarrow$ **오늘의 집중 대상**

---

## 2. 특징점 검출 (Feature Detection): 무엇이 좋은 특징인가?

매칭을 하려면 이미지 $A$의 점이 이미지 $B$의 어디에 있는지 **유일하게(Uniquely)** 식별할 수 있어야 합니다. 이를 위해 **"작은 윈도우(Window)를 움직였을 때 픽셀값이 어떻게 변하는가?"**를 살펴봅니다.

### 2.1. 세 가지 케이스 비교
1.  **Flat Region (평탄한 영역)**: 윈도우를 어느 방향으로 움직여도 변화가 없음. $\rightarrow$ **나쁜 특징점**
2.  **Edge (경계선)**: 엣지 방향으로 움직이면 변화가 없음 (**Aperture Problem**). $\rightarrow$ **나쁜 특징점** (위치가 모호함)
3.  **Corner (코너)**: 상하좌우 **모든 방향**으로 움직여도 변화가 큼. $\rightarrow$ **좋은 특징점** (위치가 확실함)

> [!abstract] 핵심 아이디어
> 좋은 특징점(Keypoint)은 **"모든 방향으로의 변화량이 큰 지점"**, 즉 **코너(Corner)**입니다.

---

## 3. Harris Corner Detector

코너를 수학적으로 정의하고 검출하는 알고리즘입니다.

### 3.1. 에너지 함수 (Energy Function)
윈도우를 $(u, v)$만큼 이동시켰을 때의 픽셀 차이 제곱합(SSD)을 정의합니다.

$$E(u, v) = \sum_{x,y} w(x,y) [I(x+u, y+v) - I(x,y)]^2$$

* $w(x,y)$: 윈도우 함수 (Box Filter 또는 **Gaussian Filter** 사용).
* $E(u, v)$가 모든 $u, v$에 대해 크다면 코너입니다.

### 3.2. 테일러 급수 근사 (Taylor Expansion)
매번 $E(u,v)$를 계산하는 것은 비효율적이므로, **테일러 급수(1차 근사)**를 사용하여 식을 단순화합니다. 작은 이동량 $(u, v)$에 대해:

$$I(x+u, y+v) \approx I(x,y) + I_x u + I_y v$$

이를 $E(u, v)$ 식에 대입하면 $I(x,y)$는 상쇄되고, 다음과 같은 **2차 형식(Quadratic Form)**을 얻습니다.

$$E(u, v) \approx \begin{bmatrix} u & v \end{bmatrix} M \begin{bmatrix} u \\ v \end{bmatrix}$$

### 3.3. Second Moment Matrix $M$
여기서 행렬 $M$은 윈도우 내 픽셀들의 그래디언트(미분값) 분포를 요약한 핵심 행렬입니다.

> [!important] Matrix M Formula
> $$M = \sum_{x,y} w(x,y) \begin{bmatrix} I_x^2 & I_x I_y \\ I_x I_y & I_y^2 \end{bmatrix}$$

이 행렬은 **대칭 행렬(Symmetric Matrix)**이므로, 고유값 분해를 통해 **타원(Ellipse)** 형태로 해석할 수 있습니다.

### 3.4. 고유값 분석을 통한 분류 (Eigenvalue Analysis)
행렬 $M$의 두 고유값 $\lambda_1, \lambda_2$는 데이터 변화의 주축(Principal Axis)의 크기를 나타냅니다.

| 조건 | 의미 | 기하학적 해석 |
| :--- | :--- | :--- |
| $\lambda_1 \approx 0, \lambda_2 \approx 0$ | **Flat** | 변화 없음. |
| $\lambda_1 \gg 0, \lambda_2 \approx 0$ | **Edge** | 한 방향으로만 변화 큼 (타원이 길쭉함). |
| $\lambda_1 \gg 0, \lambda_2 \gg 0$ | **Corner** | **두 방향 모두 변화 큼** (우리가 찾는 것). |

### 3.5. 코너 응답 함수 (Corner Response Function)
고유값 계산($\sqrt{\cdot}$ 포함)은 연산 비용이 비쌉니다. Harris는 고유값을 직접 구하지 않고, 행렬식(Det)과 대각합(Trace)을 이용한 점수 $R$을 제안했습니다.

> [!note] Response Formula
> $$R = \det(M) - k(\text{trace}(M))^2$$
> * $\det(M) = \lambda_1 \lambda_2$
> * $\text{trace}(M) = \lambda_1 + \lambda_2$
> * $k$: 실험적 상수 ($0.04 \sim 0.06$)

* $R > 0$ (큰 양수): **Corner**
* $R < 0$ (음수): **Edge**
* $|R| \approx 0$: **Flat**

### 3.6. Harris Corner의 속성
* **회전 불변 (Rotation Invariant)**: 이미지를 회전시켜도 고유값(변화량의 크기)은 변하지 않으므로 코너를 잘 찾습니다.
* **스케일 가변 (Not Scale Invariant)**: **치명적 단점**. 이미지를 확대하면 뾰족했던 코너가 완만한 엣지처럼 보여 감지할 수 없습니다.

---

## 4. 스케일 불변성 (Scale Invariance)

Harris Corner의 한계를 극복하기 위해, 이미지의 크기가 변해도 동일한 특징을 찾을 수 있는 방법이 필요합니다.

### 4.1. 블롭 검출 (Blob Detection)
코너 대신 **'블롭(Blob, 덩어리)'**을 찾습니다. 블롭 검출에 가장 좋은 필터는 **Laplacian of Gaussian (LoG)**입니다.

$$\nabla^2 G = \frac{\partial^2 G}{\partial x^2} + \frac{\partial^2 G}{\partial y^2}$$

### 4.2. 스케일 선택 (Automatic Scale Selection)
"이 블롭은 어떤 크기($\sigma$)인가?"를 알아내야 합니다.
다양한 크기($\sigma$)의 LoG 필터를 이미지에 적용했을 때, **필터 크기와 블롭 크기가 일치할 때 반응(Response)이 최대**가 됩니다. 이를 **고유 스케일(Characteristic Scale)**이라 합니다.

> [!warning] Normalization
> $\sigma$가 커질수록 미분값이 작아지므로(신호 감쇠), 공정한 비교를 위해 **스케일 정규화된 라플라시안(Scale-normalized Laplacian)**을 사용해야 합니다.
> $$\text{Norm. LoG} = \sigma^2 \nabla^2 G$$

---

## 5. SIFT (Scale Invariant Feature Transform)

SIFT는 스케일 불변성을 실용적으로 구현한 알고리즘의 정점입니다. 전체 과정은 크게 4단계로 나뉩니다.

### Step 1. Scale-space Extrema Detection (극점 탐색)
LoG는 계산이 느립니다. 따라서 **DoG (Difference of Gaussian)**로 근사합니다.
$$D(x, y, \sigma) = (G(x,y,k\sigma) - G(x,y,\sigma)) * I(x,y)$$
* **아이디어**: 서로 다른 $\sigma$로 블러링한 이미지 두 장을 **빼면(Subtract)** LoG와 유사해집니다.
* **검출**: $3 \times 3 \times 3$ 큐브(현재 픽셀 + 주변 8개 + 상위 스케일 9개 + 하위 스케일 9개 = 총 26개)에서 자신이 최대값 또는 최소값인지 확인합니다. $\rightarrow$ **후보 키포인트 선정**

### Step 2. Keypoint Localization (위치 정제)
후보 키포인트들 중 나쁜 점을 버리고 위치를 다듬습니다.
1.  **Sub-pixel Refinement**: 테일러 급수를 이용해 픽셀 사이의 **실수 좌표** 위치를 추정합니다.
2.  **Thresholding**:
    * **Low Contrast**: 명암비가 너무 낮은 점 제거 (노이즈 취약).
    * **Edge Response**: **Hessian Matrix**를 사용하여, 코너가 아니라 엣지 위에 있는 점들을 제거 (위치 불확실성 제거).

### Step 3. Orientation Assignment (방향 할당)
회전 불변성(Rotation Invariance)을 확보하는 단계입니다.
1.  키포인트 주변의 그래디언트 **방향(Orientation)**과 **크기(Magnitude)**를 계산합니다.
2.  방향 히스토그램(360도를 10도씩 36개 bin 등으로 나눔)을 생성합니다.
3.  가장 값이 큰 **주된 방향(Dominant Orientation)**을 해당 키포인트의 기준 방향으로 설정합니다.
    * 이후 모든 계산은 이 방향을 기준으로 수행되므로, 이미지가 회전해도 상관없습니다.

### Step 4. Keypoint Descriptor (기술자 생성)
마지막으로, 매칭에 사용할 **지문(Vector)**을 만듭니다.
1.  **정규화**: 키포인트 주변 $16 \times 16$ 픽셀 영역을 가져와서, 위에서 구한 주된 방향이 0도가 되도록 회전시킵니다.
2.  **그리드 분할**: $16 \times 16$ 영역을 $4 \times 4$ 크기의 **작은 블록(Sub-region) 16개**로 나눕니다.
3.  **히스토그램 생성**: 각 $4 \times 4$ 블록마다 **8방향** 그래디언트 히스토그램을 구합니다.
4.  **벡터화**:
    $$4 \times 4 \text{ (blocks)} \times 8 \text{ (directions)} = \mathbf{128} \text{ dimensions}$$
    이 128차원 벡터가 최종 SIFT Descriptor입니다.

> [!summary] SIFT의 강점
> * **Scale Invariant**: DoG Scale Space 덕분.
> * **Rotation Invariant**: Dominant Orientation 할당 덕분.
> * **Robustness**: 조명 변화(Normalization), 시점 변화, 노이즈에 강인함.

---

## 6. 특징점 매칭 (Feature Matching)

추출된 128차원 벡터들을 이용해 두 이미지 간의 짝을 찾습니다.

### 6.1. 거리 측정 (Distance Metric)
* 일반적으로 **유클리드 거리(Euclidean Distance)**, 즉 L2 Norm을 사용합니다. ($||d_1 - d_2||_2$)

### 6.2. Lowe's Ratio Test (중요)
단순히 가장 가까운 점(Nearest Neighbor)을 찾으면 오류가 많습니다.
* **방법**: 가장 가까운 이웃(1등)과의 거리와, 두 번째로 가까운 이웃(2등)과의 거리의 **비율(Ratio)**을 봅니다.
    $$\text{Ratio} = \frac{\text{Distance(1st match)}}{\text{Distance(2nd match)}}$$
* **논리**:
    * 진짜 매칭이라면 1등은 아주 가깝고 2등은 멀어야 합니다 (비율이 작음).
    * 애매한 매칭(Ambiguous)이라면 1등과 2등이 비슷할 것입니다 (비율이 1에 가까움).
* 보통 Ratio $< 0.8$ 인 경우만 매칭으로 인정하여 정확도를 높입니다.

## Questions
- 

## Merge Candidates
- [[ ]] 

## Priori Lecture
- [[ ]] 

