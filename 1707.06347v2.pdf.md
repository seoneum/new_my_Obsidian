

## Proximal Policy Optimization (PPO) 알고리즘 해설 강의 노트


## 1. 논문의 핵심 아이디어 (The Big Picture)


## 이 논문이 해결하고자 하는 문제는 무엇인가?
강화 학습(Reinforcement Learning, RL)에서 신경망 함수 근사기(Neural Network Function Approximator)를 사용하는 정책 경사(Policy Gradient) 방법론들은 데이터 효율성(Data Efficiency), 확장성(Scalability), 그리고 견고성(Robustness) 측면에서 여전히 개선의 여지가 있었습니다. 특히, 정책을 업데이트할 때 **너무 큰 변화**가 발생하여 성능이 파괴적으로 저하되는 문제를 해결하는 것이 핵심 목표였습니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="11" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="12" /></doc-source-group>


## 기존 방법론(Conventional Method)의 한계는 무엇인가?

| **기존 방법론** | **한계점** |
| --- | --- |
| **Q-러닝 (Q-learning)** | 많은 간단한 문제에서 실패하며, 이론적 이해가 부족함. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="11" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="12" /></doc-source-group> |
| **바닐라 정책 경사 (Vanilla Policy Gradient, LPG)** | 데이터 효율성과 견고성이 낮음. 하나의 데이터 샘플당 한 번의 경사 업데이트만 수행하여 비효율적임. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="7" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="12" /></doc-source-group> |
| **신뢰 영역 정책 최적화 (Trust Region Policy Optimization, TRPO)** | 데이터 효율성과 신뢰할 수 있는 성능을 제공하지만, 구현이 복잡하고 2차 근사(Quadratic Approximation)를 사용해야 하므로 대규모 모델이나 병렬 구현에 대한 확장성이 떨어짐. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="8" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="12" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="25" /></doc-source-group> |


## 이 논문의 핵심 기여(Key Contribution)를 한 문장으로 요약하면?
PPO는 TRPO의 데이터 효율성과 신뢰성을 유지하면서도, 구현이 훨씬 간단하고 1차 최적화(First-order Optimization)만을 사용하여 대규모 병렬 구현에 적합한 **클리핑된 대리 목적 함수(Clipped Surrogate Objective)**를 제안함으로써 정책 최적화의 새로운 표준을 제시했습니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="1" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="8" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="13" /></doc-source-group>



## 2. 사전 지식 (Prerequisites)


## 정책 경사 방법 (Policy Gradient Methods)
정책 $\pi_{\theta}(a|s)$의 성능을 최대화하기 위해 정책 파라미터 $\theta$를 경사 상승(Stochastic Gradient Ascent) 방식으로 업데이트하는 방법입니다. 정책 경사 추정량 $\hat{g}$는 다음과 같은 형태로 주어집니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="19" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="20" /></doc-source-group>

$$
\hat{g} = \mathbb{E}_t \left[
abla_{\theta} \log \pi_{\theta}(a_t|s_t) \hat{A}_t \right] \tag{1}
$$


## 어드밴티지 함수 (Advantage Function, $\hat{A}_t$)
특정 상태 $s_t$에서 행동 $a_t$를 취했을 때 얻는 가치(Q-value)와 해당 상태의 평균 가치(V-value)의 차이를 나타냅니다. 즉, "평균보다 얼마나 더 좋은 행동인가?"를 측정합니다.

$$
\hat{A}_t = Q(s_t, a_t) - V(s_t)
$$

PPO에서는 일반적으로 **일반화된 어드밴티지 추정(Generalized Advantage Estimation, GAE)** 의 절단된 버전(truncated version)을 사용하여 $\hat{A}_t$를 계산합니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="57" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="68" /></doc-source-group>


## 신뢰 영역 (Trust Region) 개념
정책 업데이트 시, 새로운 정책 $\pi_{\theta}$가 이전 정책 $\pi_{\theta_{old}}$로부터 너무 멀리 벗어나지 않도록 제약하는 개념입니다. TRPO는 KL-발산(KL-Divergence)을 사용하여 이 제약을 명시적인 제약 조건으로 설정합니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="23" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="25" /></doc-source-group>



## 3. 수식 및 방법론 상세 해설 (Math Deep Dive)
PPO의 핵심은 TRPO의 복잡한 2차 최적화 문제를 1차 최적화로 대체하기 위해 **대리 목적 함수(Surrogate Objective Function)**를 영리하게 변형하는 데 있습니다.


### 3.1. 정책 경사 목적 함수 (LPG)
바닐라 정책 경사 방법에서 경사 추정량 $\hat{g}$를 얻기 위해 미분하는 목적 함수입니다.

$$
L^{PG}(\theta) = \mathbb{E}_t \left[ \log \pi_{\theta}(a_t|s_t) \hat{A}_t \right] \tag{2}
$$


- **변수 정의:**
  - $\theta$: 현재 정책 $\pi$의 파라미터 벡터.
  - $\mathbb{E}_t[\dots]$: 유한한 샘플 배치에 대한 경험적 평균(Empirical Average).
  - $\pi_{\theta}(a_t|s_t)$: 상태 $s_t$에서 행동 $a_t$를 취할 확률.
  - $\hat{A}_t$: 시점 $t$에서의 어드밴티지 함수 추정량.
- **수식의 의미:** 이 목적 함수를 경사 상승시키면 정책 경사 $\hat{g}$를 얻게 됩니다. 그러나 이 목적 함수를 동일한 데이터 배치에 대해 여러 번 최적화(다중 에폭)하면 정책이 파괴적으로 크게 업데이트되는 경향이 있습니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="22" /></doc-source-group>

### 3.2. TRPO의 대리 목적 함수 (LCPI)
TRPO는 정책 비율(Probability Ratio) $r_t(\theta)$를 사용하여 목적 함수를 정의합니다.

$$
r_t(\theta) = \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
$$

이를 이용한 TRPO의 대리 목적 함수 $L^{CPI}$는 다음과 같습니다.

$$
L^{CPI}(\theta) = \mathbb{E}_t \left[ \frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)} \hat{A}_t \right] = \mathbb{E}_t \left[ r_t(\theta) \hat{A}_t \right] \tag{6}
$$


- **변수 정의:**
  - $\theta_{old}$: 업데이트 전 정책의 파라미터 벡터.
  - $r_t(\theta)$: 새로운 정책 $\pi_{\theta}$와 이전 정책 $\pi_{\theta_{old}}$의 확률 비율.
- **수식의 의미:** 이 목적 함수는 정책 업데이트의 성능을 측정하는 대리 지표입니다. $\theta = \theta_{old}$일 때 $r_t(\theta)=1$이 되며, 이때 $L^{CPI}$의 경사는 $L^{PG}$의 경사와 일치합니다.
- **한계:** $L^{CPI}$를 제약 없이 최대화하면 $r_t(\theta)$가 무한대로 커지면서 정책이 과도하게 변할 수 있습니다. TRPO는 이를 막기 위해 KL-발산 제약 조건(Constraint)을 추가합니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="24" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="36" /></doc-source-group>

### 3.3. PPO의 클리핑된 대리 목적 함수 (LCLIP)
PPO는 TRPO의 복잡한 제약 조건을 **목적 함수 자체에 통합**하여 1차 최적화만으로 안정성을 확보합니다.

$$
L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right] \tag{7}
$$


- **변수 정의:**
  - $\epsilon$: 클리핑 하이퍼파라미터 (예: $\epsilon=0.2$).
  - $\text{clip}(r, a, b)$: $r$의 값을 $[a, b]$ 범위로 제한하는 함수.
- **수식의 의미 (직관적 설명):**

- **전개 과정 (왜 $\min$을 사용하는가?):**
  - **Case 1: $\hat{A}_t > 0$ (좋은 행동)**
  - 정책 업데이트가 $r_t(\theta)$를 $1+\epsilon$보다 크게 만들려고 할 때, 두 번째 항이 $r_t(\theta)$를 $1+\epsilon$으로 클리핑합니다.
  - $\min$ 연산은 $r_t(\theta) \hat{A}_t$와 $(1+\epsilon) \hat{A}_t$ 중 작은 값을 선택합니다. $r_t(\theta) > 1+\epsilon$이므로, $\min$은 $(1+\epsilon) \hat{A}_t$를 선택합니다.
  - **결과:** 정책이 너무 좋아지더라도, $1+\epsilon$ 이상의 이득은 무시됩니다. 이는 정책이 과도하게 업데이트되는 것을 방지합니다.
  - **Case 2: $\hat{A}_t < 0$ (나쁜 행동)**
  - 정책 업데이트가 $r_t(\theta)$를 $1-\epsilon$보다 작게 만들려고 할 때, 두 번째 항이 $r_t(\theta)$를 $1-\epsilon$으로 클리핑합니다.
  - $\min$ 연산은 $r_t(\theta) \hat{A}_t$와 $(1-\epsilon) \hat{A}_t$ 중 작은 값을 선택합니다. $\hat{A}_t$가 음수이므로, $r_t(\theta) < 1-\epsilon$일 때 $r_t(\theta) \hat{A}_t$가 더 큰 음수(더 작은 값)가 됩니다.
  - **결과:** 정책이 나쁜 행동에 대해 $1-\epsilon$보다 더 나빠지면, 그 손실은 그대로 반영됩니다.

- **구현 관점:** $L^{CLIP}$는 미분 가능한 함수이므로, 자동 미분(Automatic Differentiation) 소프트웨어를 사용하여 경사를 계산하고 SGD(Stochastic Gradient Descent) 또는 Adam 최적화 기법으로 여러 에폭 동안 최적화할 수 있습니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="46" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="55" /></doc-source-group>

### 3.4. 전체 목적 함수 (LCLIP+VF+S)
실제 구현에서는 정책 목적 함수에 가치 함수 오차 항(Value Function Error Term)과 탐색을 장려하는 엔트로피 보너스(Entropy Bonus)를 추가합니다.

$$
L^{CLIP+VF+S}(\theta) = \mathbb{E}_t \left[ L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S[\pi_{\theta}](s_t) \right] \tag{9}
$$


- **추가 변수 정의:**
  - $L^{VF}(\theta)$: 가치 함수 오차 항. 보통 제곱 오차 손실(Squared-Error Loss) $ (V_{\theta}(s_t) - V_t^{targ})^2 $를 사용합니다.
  - $S[\pi_{\theta}](s_t)$: 정책 $\pi_{\theta}$의 엔트로피 보너스. 정책의 무작위성(탐색)을 장려합니다.
  - $c_1, c_2$: 각 항의 중요도를 조절하는 계수.
- **수식의 의미:** 이 결합된 목적 함수를 최대화함으로써, 정책은 보상을 극대화하고(LCLIP), 가치 함수는 정확해지며(LVF), 충분한 탐색(S)을 유지하게 됩니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="65" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="66" /></doc-source-group>


## 4. 실험 결과 및 해석 (Experiments & Analysis)


## 저자들이 주장을 입증하기 위해 어떤 실험을 설계했는가?
PPO의 성능과 안정성을 입증하기 위해 두 가지 주요 벤치마크에서 실험을 수행했습니다.

1. **연속 제어 작업 (Continuous Control Tasks):** OpenAI Gym의 MuJoCo 물리 엔진을 사용하는 7가지 시뮬레이션 로봇 제어 문제. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="76" /></doc-source-group>
2. **Atari 게임 (Arcade Learning Environment):** 이산 행동 공간을 가진 49가지 Atari 게임. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="101" /></doc-source-group>

특히, PPO의 핵심인 **클리핑된 목적 함수**의 효과를 검증하기 위해 다양한 대리 목적 함수 변형들을 비교했습니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="61" /></doc-source-group>


## 그래프나 표가 의미하는 바를 구체적인 수치와 함께 해석하십시오.


### 4.1. 대리 목적 함수 비교 (연속 제어)
저자들은 클리핑(Clipping) 방식과 적응적 KL 페널티(Adaptive KL Penalty) 방식을 비교했습니다. 결과는 정규화된 평균 점수(Average Normalized Score)로 제시됩니다 (무작위 정책 0점, 최고 성능 1점). <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="79" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="80" /></doc-source-group>


| **알고리즘 변형** | **평균 정규화 점수** |
| --- | --- |
| 클리핑, $\epsilon=0.2$ ($L^{CLIP}$) | **0.82** |
| 클리핑, $\epsilon=0.1$ | 0.76 |
| 클리핑, $\epsilon=0.3$ | 0.70 |
| 적응적 KL, $d_{targ}=0.01$ ($L^{KLPEN}$) | 0.74 |
| 클리핑 또는 페널티 없음 (No clipping or penalty) | -0.39 |


- **해석:**
  - **클리핑의 우수성:** 클리핑된 대리 목적 함수($L^{CLIP}$)가 적응적 KL 페널티($L^{KLPEN}$)보다 더 나은 성능(0.82 vs 0.74)을 보였습니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="82" /></doc-source-group>
  - **안정성 입증:** 클리핑이나 페널티가 없는 경우(-0.39)는 초기 무작위 정책보다도 훨씬 나쁜 결과를 초래했습니다. 이는 정책 업데이트 크기를 제한하는 메커니즘이 RL 알고리즘의 안정성에 **필수적**임을 입증합니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="81" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="82" /></doc-source-group>
  - **하이퍼파라미터 민감도:** $\epsilon=0.2$가 최적의 성능을 보였으며, $\epsilon$ 값에 따라 성능이 변동하지만, 전반적으로 클리핑 방식이 견고함을 보여줍니다.

### 4.2. 다른 알고리즘과의 비교 (연속 제어)
PPO(Clip)를 TRPO, CEM, A2C 등 기존의 효과적인 연속 제어 알고리즘들과 비교했습니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="83" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="91" /></doc-source-group>


- **결과:** Figure 3의 학습 곡선에서 PPO는 거의 모든 연속 제어 환경에서 기존 방법론들을 능가하는 성능을 보였습니다. 특히, TRPO와 유사하거나 더 나은 성능을 보이면서도 구현 복잡도는 훨씬 낮다는 장점을 가집니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="91" /></doc-source-group>

### 4.3. Atari 게임 성능 비교
PPO를 A2C와 ACER(Sample-Efficient Actor-Critic with Experience Replay)과 비교했습니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="99" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="101" /></doc-source-group>


| **평가 지표** | **A2C 승리 게임 수** | **ACER 승리 게임 수** | **PPO 승리 게임 수** | **동점** |
| --- | --- | --- | --- | --- |
| 전체 훈련 기간 평균 보상 | 1 | 18 | **30** | 0 |
| 마지막 100 에피소드 평균 보상 | 1 | 28 | **19** | 1 |


- **해석:**
  - **빠른 학습:** PPO는 전체 훈련 기간 동안의 평균 보상(빠른 학습 속도 선호)에서 30승을 거두며 A2C와 ACER을 압도했습니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="104" /></doc-source-group>
  - **최종 성능:** 마지막 100 에피소드 평균 보상(최종 성능 선호)에서는 ACER(28승)이 PPO(19승)보다 우세했습니다. 이는 ACER이 경험 리플레이(Experience Replay)를 사용하여 데이터 효율성을 높인 반면, PPO는 온라인(On-policy) 방식의 한계로 인해 최종 수렴 성능에서 약간 밀릴 수 있음을 시사합니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="104" /><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="120" /></doc-source-group>
  - **결론:** PPO는 A2C보다 훨씬 우수하며, ACER과 비교했을 때 구현의 단순성 대비 매우 경쟁력 있는 성능을 제공합니다.


## 5. 한계점 및 비판적 사고 (Critical Thinking)


## 이 방법론의 잠재적 약점이나 향후 연구 과제(Future Work)는 무엇인가?
1. **온라인(On-policy) 학습의 한계:**
  - PPO는 기본적으로 온-정책 알고리즘입니다. 즉, 데이터를 수집한 정책($\pi_{\theta_{old}}$)으로만 목적 함수를 최적화할 수 있습니다. 비록 여러 에폭의 미니배치 업데이트를 허용하여 데이터 효율성을 높였지만, 여전히 오프-정책(Off-policy) 알고리즘(예: ACER, Q-learning)만큼 데이터를 재활용하지 못합니다.
  - **향후 연구:** PPO의 안정성을 유지하면서 오프-정책 학습의 장점(데이터 재활용 극대화)을 결합하는 연구가 필요합니다.

2. **클리핑 하이퍼파라미터 $\epsilon$의 민감도:**
  - 실험 결과에서 $\epsilon=0.2$가 최적의 성능을 보였으며, $\epsilon$ 값의 변화에 따라 성능이 민감하게 변동했습니다 (0.82 $\rightarrow$ 0.70). <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="82" /></doc-source-group>
  - **향후 연구:** $\epsilon$ 값을 문제나 학습 단계에 따라 자동으로 조정하는 메커니즘을 개발하여 하이퍼파라미터 튜닝의 부담을 줄일 수 있습니다.

3. **적응적 KL 페널티의 성능 문제:**
  - 논문에서 대안으로 제시된 적응적 KL 페널티($L^{KLPEN}$)는 클리핑 방식($L^{CLIP}$)보다 성능이 낮았습니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="51" /></doc-source-group>
  - **비판적 사고:** TRPO의 이론적 근거는 KL 제약에 있지만, 실제 구현 및 1차 최적화 환경에서는 클리핑이라는 단순한 휴리스틱(Heuristic)이 더 잘 작동했습니다. 이는 이론적 완벽성보다 실용적인 안정성이 더 중요할 수 있음을 보여줍니다. 클리핑 방식이 왜 KL 페널티보다 더 견고한지에 대한 심층적인 이론적 분석이 필요합니다.

4. **복잡한 환경에서의 확장성:**
  - PPO는 3D 휴머노이드 제어와 같은 고차원 문제에서 뛰어난 성능을 보였지만, 매우 희소한 보상(Sparse Reward)을 가진 환경(예: Montezuma's Revenge)에서는 여전히 0점의 평균 보상을 기록하는 등 한계를 보였습니다. <doc-source-group><doc-source requestId="e740b4b3-f229-449f-a649-dc1627b54ef8" index="143" /></doc-source-group>
  - **향후 연구:** PPO를 탐색 전략(Exploration Strategy)이나 계층적 RL(Hierarchical RL)과 결합하여 복잡하고 탐색이 어려운 환경에서의 성능을 개선해야 합니다.

