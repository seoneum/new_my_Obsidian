---
type: reference
title: R - Proximal Policy Optimization Algorithms
created: 2026-01-26
updated: 2026-01-26T16:29:27
source_url: ""
tags:
  - reference
  - tagging/needed
author:
  - "[[OpenAI]]"
---
# Proximal Policy Optimization Algorithms

## ğŸ“‹ Metadata
- URL: 
- Author: [[OpenAI]]
- Date: 2026-01-26

---

## ğŸ“ Summary

### Abstract
- ìƒˆë¡œìš´ Policy Grandient ë°©ë²•ë¡  ì œì‹œ.
- Stochastic Grandient Ascentë¥¼ ì‚¬ìš©í•˜ì—¬ Surrogate ëª©ì  í•¨ìˆ˜ë¥¼ ìµœëŒ€í™” í•¨.
- ê¸°ì¡´ì˜ ë°©ì‹ì€ sample í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ ì—…ë°ì´íŠ¸ë¥¼ í•¨. í•˜ì§€ë§Œ PPOì˜ ëª©ì  í•¨ìˆ˜ëŠ” ì—¬ëŸ¬ Epochì— ê±¸ì¹œ ë¯¸ë‹ˆë°°ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ í•¨. (ë¯¸ë‹ˆ ë°°ì¹˜ ì—…ë°ì´íŠ¸ê°€ ë­ì§€? ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ìª¼ê°œì„œ í•œë‹¤ëŠ”ê±´ê°€?)
### Introduction
- ê¸°ì¡´ ë°©ë²•ë¡ ì˜ í•œê³„
	- Q-learning : ê°„ë‹¨í•œ ë¬¸ì œì—ì„œë„ ì‹¤íŒ¨ + ì´ë¡ ì  ì´í•´ ë¶€ì¡±
	- Vanila Policy Grandient : ë°ì´í„° íš¨ìœ¨ì„±,ê²¬ê³ ì„± ë‚®ìŒ. í•˜ë‚˜ì˜ ìƒ˜í”Œì— í•˜ë‚˜ì˜ ì—…ë°ì´íŠ¸
	- Trust Region Policy Optimization : êµ¬í˜„ ë³µì¡. Quadratic Approximation ì‚¬ìš©ìœ¼ë¡œ í™•ì¥ì„± ë‚®ìŒ
	- TRPOì—ì„œ ë°œì „í•œ ë°©ë²•ìœ¼ë¡œ ë” ê°„ë‹¨í•˜ê³  ë²”ìš©ì ì´ê³  ëœ ë³µì¡í•©.
	- First-order-optimizationë§Œ ì‚¬ìš©í•˜ë©´ì„œ TRPOì˜ ì•ˆì •ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë‹¬ì„±.
	- **Cliped probability ratios**ë¥¼ ì‚¬ìš©í•˜ëŠ” ëª©ì  í•¨ìˆ˜ ê°œë°œ.
	- Cliped probability ratios : í•˜ë°©(ë¹„ê´€ì  ì¶”ì •ì¹˜)ì„ ìƒì„±. 
### Background
- Policy Grandient Methods
ì •ì±… $\pi_{\theta}(a|s)$ì˜ ì„±ëŠ¥ì„ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´ ì •ì±… íŒŒë¼ë¯¸í„° $\theta$ë¥¼ ê²½ì‚¬ ìƒìŠ¹(Stochastic Gradient Ascent) ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì •ì±… ê²½ì‚¬ ì¶”ì •ëŸ‰ $\hat{g}$ëŠ” ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¡œ ì£¼ì–´ì§‘ë‹ˆë‹¤. 
$$
\hat{g}â€‹=\hat{E_{t}}â€‹[âˆ‡_{Î¸}â€‹logÏ€_{Î¸}â€‹(a_{t}â€‹âˆ£s_{t}â€‹)\hat{A^t}â€‹]
$$
$\hat{g}$ : Grandientì˜ ì¶”ì •ê°’. ì´ ë°©í–¥ìœ¼ë¡œ parameterì—…ë°ì´íŠ¸.
$âˆ‡_{\theta}â€‹logÏ€_{\theta}(a_{t}â€‹âˆ£s_{t}â€‹)$: "ë¡œê·¸ í™•ë¥ ì˜ ê¸°ìš¸ê¸°"ì…ë‹ˆë‹¤.
	$Ï€Î¸â€‹(a_{t}âˆ£s_{t}â€‹)$ : ìƒíƒœ $S_t$ì—ì„œ í–‰ë™ $a_t$ë¥¼ ì„ íƒí•  í™•ë¥ .
	$\theta$ : ì‹ ê²½ë§ì˜ parameter
	ì´ í•­ì€ íŠ¹ì • í–‰ë™ì„ í•  í™•ë¥ ì„ ì¡°ì •í•˜ê¸° ìœ„í•´ $\theta$ë¥¼ ì¡°ì •í•¨.
$\hat{A_{t}}$ : Advantage í•¨ìˆ˜ì„. ì—…ë°ì´íŠ¸ì˜ í¬ê¸°, ë°©í–¥(ì–‘/ìŒ)ì„ ì •í•˜ëŠ” ê°€ì¤‘ì¹˜

- Advantage í•¨ìˆ˜
 $$\hat{A_{t}} = Q(s_{t})â€‹ - A(s_{t})$$
	- $\hat{A}_t$ : í–‰ë™ $a_t$ì˜ ìƒëŒ€ì  ê°€ì¹˜. (ì–‘ìˆ˜ë©´ ê°•í™”, ìŒìˆ˜ë©´ ì–µì œ).
	- $Q(s_t, a_t)$ : í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜. ìƒíƒœ $s_t$ì—ì„œ í–‰ë™ $a_t$ë¥¼ í–ˆì„ ë•Œ ì–»ì€ ì‹¤ì œ(ë˜ëŠ” ì˜ˆì¸¡) ë³´ìƒ ì´í•©. 
	- $V(s_t)$ : ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ (Baseline). ìƒíƒœ $s_t$ì—ì„œ ê¸°ëŒ€ë˜ëŠ” í‰ê·  ë³´ìƒ. 
	- $Q - V$ : í–‰ë™ì˜ ê²°ê³¼ì—ì„œ í‰ê· ì„ ëºŒìœ¼ë¡œì¨ í•™ìŠµì˜ ë¶„ì‚°(Variance)ì„ ì¤„ì´ëŠ” ì—­í• .
---

## ğŸ’¡ Key Points
1. 
2. 
3. 

---

## ğŸ“ Quotes
> 

---

## ğŸ”— Related
- [[ ]]

