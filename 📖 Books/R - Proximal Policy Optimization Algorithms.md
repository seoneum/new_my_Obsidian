---
type: reference
title: R - Proximal Policy Optimization Algorithms
created: 2026-01-26
updated: 2026-01-26T16:29:27
source_url: ""
tags:
  - reference
  - tagging/needed
author:
  - "[[OpenAI]]"
---
# Proximal Policy Optimization Algorithms

## ğŸ“‹ Metadata
- URL: 
- Author: [[OpenAI]]
- Date: 2026-01-26

---

## ğŸ“ Summary

### Abstract
- ìƒˆë¡œìš´ Policy Gradient ë°©ë²•ë¡  ì œì‹œ.
- Stochastic Gradient Ascentë¥¼ ì‚¬ìš©í•˜ì—¬ Surrogate ëª©ì  í•¨ìˆ˜ë¥¼ ìµœëŒ€í™” í•¨.
- ê¸°ì¡´ì˜ ë°©ì‹ì€ sample í•˜ë‚˜ë‹¹ í•˜ë‚˜ì˜ ì—…ë°ì´íŠ¸ë¥¼ í•¨. í•˜ì§€ë§Œ PPOì˜ ëª©ì  í•¨ìˆ˜ëŠ” ì—¬ëŸ¬ Epochì— ê±¸ì¹œ ë¯¸ë‹ˆë°°ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•¨.
	- *ë¯¸ë‹ˆë°°ì¹˜ ì—…ë°ì´íŠ¸ë€?*: ì „ì²´ ë°ì´í„°ë¥¼ ì‘ì€ ë¬¶ìŒ(ë°°ì¹˜)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì—¬ëŸ¬ ë²ˆ í•™ìŠµí•˜ëŠ” ê²ƒ. ê¸°ì¡´ ë°©ë²•ì€ í•œ ë²ˆ ì“´ ë°ì´í„°ëŠ” ë²„ë ¤ì•¼ í–ˆì§€ë§Œ(On-policy), PPOëŠ” 'ì•ˆì „ ì¥ì¹˜' ë•ë¶„ì— ê°™ì€ ë°ì´í„°ë¥¼ ëª‡ ë²ˆ ë” ì¬ì‚¬ìš©í•´ë„ í•™ìŠµì´ ë§ê°€ì§€ì§€ ì•ŠìŒ.

### Introduction
- **ê¸°ì¡´ ë°©ë²•ë¡ ì˜ í•œê³„**
	- **Q-learning**: ê°„ë‹¨í•œ ë¬¸ì œì—ì„œë„ ì‹¤íŒ¨í•˜ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë©°, ì´ë¡ ì  ì´í•´ê°€ ë¶€ì¡±í•¨.
	- **Vanilla Policy Gradient**: ë°ì´í„° íš¨ìœ¨ì„±ì´ ë‚®ê³  ê²¬ê³ í•˜ì§€ ì•ŠìŒ(Robustness ë¶€ì¡±). í•˜ë‚˜ì˜ ìƒ˜í”Œë¡œ ë”± í•œ ë²ˆë§Œ ì—…ë°ì´íŠ¸ ê°€ëŠ¥.
	- **Trust Region Policy Optimization (TRPO)**: ìˆ˜í•™ì ìœ¼ë¡œëŠ” í›Œë¥­í•˜ì§€ë§Œ êµ¬í˜„ì´ ë§¤ìš° ë³µì¡í•˜ê³  ê³„ì‚°ëŸ‰ì´ ë§ìŒ(Quadratic Approximation ì‚¬ìš©).
- **PPOì˜ ì œì•ˆ**
	- TRPOì˜ ì¥ì (ì•ˆì •ì„±, íš¨ìœ¨ì„±)ì€ ê°€ì ¸ì˜¤ë˜, í›¨ì”¬ êµ¬í˜„í•˜ê¸° ì‰½ê³  ë²”ìš©ì ì¸ ë°©ë²•.
	- 1ì°¨ ë¯¸ë¶„(First-order optimization)ë§Œ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ì´ ë¹ ë¦„.
	- **Clipped Probability Ratios**ë¼ëŠ” ìƒˆë¡œìš´ ì•„ì´ë””ì–´ ë„ì….
	- ì´ëŠ” ì •ì±…ì´ ë„ˆë¬´ ê¸‰ê²©í•˜ê²Œ ë³€í•˜ëŠ” ê²ƒì„ ë§‰ì•„ì£¼ëŠ” "ê°€ë“œë ˆì¼" ì—­í• ì„ í•¨.

### Background
- **Policy Gradient Methods**
	ì •ì±… $\pi_{\theta}(a|s)$ì˜ ì„±ëŠ¥ì„ ìµœëŒ€í™”í•˜ê¸° ìœ„í•´ ì •ì±… íŒŒë¼ë¯¸í„° $\theta$ë¥¼ ê²½ì‚¬ ìƒìŠ¹(Stochastic Gradient Ascent) ë°©ì‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì •ì±… ê²½ì‚¬ ì¶”ì •ëŸ‰ $\hat{g}$ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 
	$$
	\hat{g}â€‹=\hat{E}_{t}â€‹[\nabla_{\theta}â€‹\log\pi_{\theta}â€‹(a_{t}â€‹âˆ£s_{t}â€‹)\hat{A}_tâ€‹]
	$$
	- $\hat{g}$: Gradient(ê¸°ìš¸ê¸°)ì˜ ì¶”ì •ê°’. ì´ ë°©í–¥ìœ¼ë¡œ íŒŒë¼ë¯¸í„° $\theta$ë¥¼ ì¡°ê¸ˆì”© ì´ë™ì‹œí‚µë‹ˆë‹¤.
	- $\nabla_{\theta}â€‹\log\pi_{\theta}(a_{t}â€‹âˆ£s_{t}â€‹)$: "ë¡œê·¸ í™•ë¥ ì˜ ê¸°ìš¸ê¸°". í˜„ì¬ ì •ì±…ì—ì„œ í•´ë‹¹ í–‰ë™ì„ í•  í™•ë¥ ì´ ë†’ì•„ì§€ëŠ” ë°©í–¥ì„ ê°€ë¦¬í‚µë‹ˆë‹¤.
		- $\pi_\theta(a_{t}âˆ£s_{t})$: ìƒíƒœ $s_t$ì—ì„œ í–‰ë™ $a_t$ë¥¼ ì„ íƒí•  í™•ë¥ .
		- $\theta$: ì‹ ê²½ë§ì˜ íŒŒë¼ë¯¸í„° (ê°€ì¤‘ì¹˜).
	- $\hat{A}_{t}$: **Advantage í•¨ìˆ˜**. ì´ í–‰ë™ì´ 'ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€'ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê°€ì¤‘ì¹˜ì…ë‹ˆë‹¤.

- **Advantage í•¨ìˆ˜**
	 $$
	 \hat{A}_{t} = Q(s_{t}, a_t)â€‹ - V(s_{t})
	 $$
	- $\hat{A}_t$: í–‰ë™ $a_t$ì˜ ìƒëŒ€ì  ê°€ì¹˜. (ì–‘ìˆ˜ë©´ í‰ì†Œë³´ë‹¤ ì¢‹ì€ í–‰ë™ â†’ í™•ë¥  ë†’ì„, ìŒìˆ˜ë©´ ë‚˜ìœ í–‰ë™ â†’ í™•ë¥  ë‚®ì¶¤).
	- $Q(s_t, a_t)$: í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜. ìƒíƒœ $s_t$ì—ì„œ í–‰ë™ $a_t$ë¥¼ í–ˆì„ ë•Œ ì–»ì„ ê²ƒìœ¼ë¡œ ê¸°ëŒ€ë˜ëŠ” ë³´ìƒì˜ ì´í•©.
	- $V(s_t)$: ìƒíƒœ ê°€ì¹˜ í•¨ìˆ˜ (Baseline). ìƒíƒœ $s_t$ì—ì„œ í‰ê· ì ìœ¼ë¡œ ê¸°ëŒ€ë˜ëŠ” ë³´ìƒ.
	- **ì˜ë¯¸**: "ë‚´ê°€ í•œ í–‰ë™($Q$)ì´ í‰ê· ($V$)ë³´ë‹¤ ì–¼ë§ˆë‚˜ ë” ë‚˜ì€ê°€?"ë¥¼ ì¸¡ì •í•˜ì—¬ í•™ìŠµì˜ ë¶„ì‚°(Variance)ì„ ì¤„ì—¬ì¤ë‹ˆë‹¤.

### 3. Clipped Surrogate Objective (í•µì‹¬ ë‚´ìš©)
PPOì˜ ê°€ì¥ í•µì‹¬ì ì¸ ê³µì‹ì…ë‹ˆë‹¤. ê¸°ì¡´ TRPOëŠ” ë³µì¡í•œ ì œì•½ì¡°ê±´ì„ ê±¸ì—ˆì§€ë§Œ, PPOëŠ” **Clipping(ìë¥´ê¸°)** ì´ë¼ëŠ” ë‹¨ìˆœí•œ ë°©ë²•ìœ¼ë¡œ ë¹„ìŠ·í•œ íš¨ê³¼ë¥¼ ëƒ…ë‹ˆë‹¤.

#### 1) í™•ë¥  ë¹„ìœ¨ (Probability Ratio) $r_t(\theta)$
ë¨¼ì €, ìƒˆë¡œìš´ ì •ì±…ê³¼ ì˜›ë‚  ì •ì±…ì˜ ë¹„ìœ¨ì„ ì •ì˜í•©ë‹ˆë‹¤.
$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
$$
- $r_t(\theta)$: "í™•ë¥  ë¹„ìœ¨"ì…ë‹ˆë‹¤.
	- $r_t > 1$: ìƒˆë¡œìš´ ì •ì±…ì´ í•´ë‹¹ í–‰ë™ì„ ë” ë§ì´ í•˜ë ¤ê³  í•¨.
	- $r_t < 1$: ìƒˆë¡œìš´ ì •ì±…ì´ í•´ë‹¹ í–‰ë™ì„ ëœ í•˜ë ¤ê³  í•¨.
	- $r_t = 1$: ì •ì±…ì´ ë³€í•˜ì§€ ì•ŠìŒ.
- ì´ ë¹„ìœ¨ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” **Log í™•ë¥  ë¯¸ë¶„($\nabla \log \pi$)** ì„ **ë¹„ìœ¨($\frac{\pi}{\pi_{old}}$)** í˜•íƒœë¡œ ë³€í˜•í•˜ì—¬ ê³„ì‚°í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤ (TRPOì˜ Surrogate Objective).

#### 2) PPOì˜ ëª©ì  í•¨ìˆ˜ $L^{CLIP}$
ì •ì±…ì´ ë„ˆë¬´ ê¸‰ê²©í•˜ê²Œ ë³€í•´ì„œ ë§ê°€ì§€ëŠ” ê²ƒì„ ë§‰ê¸° ìœ„í•´, $r_t(\theta)$ê°€ $1$ì—ì„œ ë„ˆë¬´ ë©€ì–´ì§€ì§€ ì•Šë„ë¡ **Clip(ìë¥´ê¸°)** ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
$$
L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t \right) \right]
$$

ì´ ê³µì‹ì´ PPOì˜ ì „ë¶€ì…ë‹ˆë‹¤. í•˜ë‚˜ì”© ëœ¯ì–´ë´…ì‹œë‹¤.
- **$\epsilon$ (Epsilon)**: í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ, ë³´í†µ $0.1$ ë˜ëŠ” $0.2$ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì •ì±…ì´ ë³€í•  ìˆ˜ ìˆëŠ” í—ˆìš© ë²”ìœ„ë¥¼ ëœ»í•©ë‹ˆë‹¤. (ì˜ˆ: $0.2$ë¼ë©´ ê¸°ì¡´ í™•ë¥ ì˜ $0.8$ë°° ~ $1.2$ë°°ê¹Œì§€ë§Œ ì¸ì •).
- **$\text{clip}(r_t, 1-\epsilon, 1+\epsilon)$**: ë¹„ìœ¨ $r_t$ê°€ $1-\epsilon$ë³´ë‹¤ ì‘ìœ¼ë©´ $1-\epsilon$ìœ¼ë¡œ, $1+\epsilon$ë³´ë‹¤ í¬ë©´ $1+\epsilon$ìœ¼ë¡œ ê°•ì œë¡œ ë§ì¶¥ë‹ˆë‹¤.
- **$\min( \dots )$**: ì›ë˜ì˜ ì‹(ì™¼ìª½)ê³¼ ìë¥¸ ì‹(ì˜¤ë¥¸ìª½) ì¤‘ **ë” ì‘ì€ ê°’(ë¹„ê´€ì ì¸ ê°’)** ì„ ì„ íƒí•©ë‹ˆë‹¤.
	- **ì™œ ì‘ì€ ê°’ì„ ì„ íƒí•˜ë‚˜ìš”?** ë„ˆë¬´ ë‚™ê´€ì ìœ¼ë¡œ í•™ìŠµí•´ì„œ ì •ì±…ì´ ë§ê°€ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. ì´ë¥¼ "ë¹„ê´€ì  í•˜í•œ(Pessimistic Lower Bound)"ì´ë¼ê³  í•©ë‹ˆë‹¤.

**ì§ê´€ì  ì„¤ëª… (Intuition):**
- **Advantageê°€ ì–‘ìˆ˜ì¼ ë•Œ ($\hat{A} > 0$)**: í–‰ë™ì´ ì¢‹ì•˜ìœ¼ë¯€ë¡œ í™•ë¥ ì„ ë†’ì—¬ì•¼ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ $r_t$ê°€ $1+\epsilon$ì„ ë„˜ì–´ì„œ ë„ˆë¬´ í¬ê²Œ ë†’ì´ë ¤ê³  í•˜ë©´, Clipì— ê±¸ë ¤ì„œ ë” ì´ìƒ ë³´ìƒì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. (ì¦‰, "ì ë‹¹íˆ ì˜¬ë ¤ë¼").
- **Advantageê°€ ìŒìˆ˜ì¼ ë•Œ ($\hat{A} < 0$)**: í–‰ë™ì´ ë‚˜ë¹´ìœ¼ë¯€ë¡œ í™•ë¥ ì„ ë‚®ì¶°ì•¼ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ $r_t$ê°€ $1-\epsilon$ë³´ë‹¤ ë„ˆë¬´ ì‘ê²Œ ë‚®ì¶”ë ¤ê³  í•˜ë©´, ì—­ì‹œ Clipì— ê±¸ë¦½ë‹ˆë‹¤. (ì¦‰, "ì ë‹¹íˆ ë‚®ì¶°ë¼").
- **ê²°ê³¼**: ì •ì±… ì—…ë°ì´íŠ¸ê°€ í•œ ë²ˆì— ë„ˆë¬´ í¬ê²Œ ì¼ì–´ë‚˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ì—¬ í•™ìŠµì„ ë§¤ìš° ì•ˆì •ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.

---

### 4. Adaptive KL Penalty Coefficient (ëŒ€ì•ˆ ë°©ë²•)
Clipping ëŒ€ì‹  ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë˜ ë‹¤ë¥¸ ë°©ë²•ì…ë‹ˆë‹¤. ì •ì±…ì˜ ë³€í™”ëŸ‰(KL Divergence)ì„ ì¸¡ì •í•´ì„œ íŒ¨ë„í‹°ë¥¼ ì¤ë‹ˆë‹¤.

$$
L^{KLPEN}(\theta) = \hat{\mathbb{E}}_t \left[ r_t(\theta)\hat{A}_t - \beta \text{KL}[\pi_{\theta_{old}}(\cdot|s_t), \pi_\theta(\cdot|s_t)] \right]
$$

- $\text{KL}[\dots]$: ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë°œì‚°(Kullback-Leibler Divergence). ë‘ í™•ë¥ ë¶„í¬(ì˜› ì •ì±… vs ìƒˆ ì •ì±…)ê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ ì¸¡ì •í•˜ëŠ” 'ê±°ë¦¬' ê°œë…ì…ë‹ˆë‹¤.
- $\beta$: íŒ¨ë„í‹° ê³„ìˆ˜ì…ë‹ˆë‹¤.
	- ì •ì±…ì´ ë„ˆë¬´ ë§ì´ ë³€í–ˆë‹¤ ì‹¶ìœ¼ë©´($KL > d_{targ}$) $\rightarrow$ $\beta$ë¥¼ í‚¤ì›Œì„œ ë²Œì ì„ ë” ì„¸ê²Œ ì¤ë‹ˆë‹¤.
	- ì •ì±…ì´ ë„ˆë¬´ ì¡°ê¸ˆ ë³€í–ˆë‹¤ ì‹¶ìœ¼ë©´($KL < d_{targ} / 1.5$) $\rightarrow$ $\beta$ë¥¼ ì¤„ì—¬ì„œ ë” ììœ ë¡­ê²Œ ë‘¡ë‹ˆë‹¤.
- PPO ë…¼ë¬¸ì—ì„œëŠ” Clipping ë°©ë²•($L^{CLIP}$)ì´ ë” ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  ê²°ë¡ ì§€ì—ˆìŠµë‹ˆë‹¤.

---

### 5. Final Loss Function (ìµœì¢… ì†ì‹¤ í•¨ìˆ˜)
ì‹¤ì œ PPO í•™ìŠµì—ì„œëŠ” ì •ì±…ë§Œ í•™ìŠµí•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê°€ì¹˜ í•¨ìˆ˜(Value Function)ë„ ê°™ì´ í•™ìŠµí•´ì•¼ í•˜ê³ , íƒí—˜(Exploration)ì„ ìœ„í•´ ì—”íŠ¸ë¡œí”¼(Entropy)ë„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.

ìµœì¢…ì ìœ¼ë¡œ ìš°ë¦¬ê°€ ìµœëŒ€í™”í•´ì•¼ í•  ëª©ì  í•¨ìˆ˜ëŠ” ì´ 3ê°€ì§€ë¥¼ í•©ì¹œ ê²ƒì…ë‹ˆë‹¤.

$$
L_t^{CLIP+VF+S}(\theta) = \hat{\mathbb{E}}_t \left[ L_t^{CLIP}(\theta) - c_1 L_t^{VF}(\theta) + c_2 S[\pi_\theta](s_t) \right]
$$

1. **$L_t^{CLIP}(\theta)$**: ìœ„ì—ì„œ ë³¸ PPOì˜ í•µì‹¬ ì •ì±… ì†ì‹¤ í•¨ìˆ˜. (ì •ì±…ì„ ì¢‹ê²Œ ë§Œë“¦)
2. **$L_t^{VF}(\theta)$**: ê°€ì¹˜ í•¨ìˆ˜ ì†ì‹¤ (Value Function Loss).
   $$ L_t^{VF} = (V_\theta(s_t) - V_t^{target})^2 $$
   - ë‚´ ì‹ ê²½ë§ì´ ì˜ˆì¸¡í•œ ê°€ì¹˜($V_\theta$)ê°€ ì‹¤ì œ(ë˜ëŠ” íƒ€ê²Ÿ) ê°€ì¹˜($V^{target}$)ì™€ ë¹„ìŠ·í•´ì§€ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤. (Mean Squared Error).
3. **$S[\pi_\theta](s_t)$**: ì—”íŠ¸ë¡œí”¼ ë³´ë„ˆìŠ¤ (Entropy Bonus).
   - ì •ì±…ì˜ ì—”íŠ¸ë¡œí”¼(ë¬´ì§ˆì„œë„)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
   - ì´ ê°’ì´ í¬ë©´ í™•ë¥  ë¶„í¬ê°€ í¼ì ¸ ìˆì–´ì„œ ë‹¤ì–‘í•œ í–‰ë™ì„ ì‹œë„í•©ë‹ˆë‹¤.
   - í•™ìŠµ ì´ˆê¸°ì— ë„ˆë¬´ ë¹¨ë¦¬ í•œ ê°€ì§€ í–‰ë™ìœ¼ë¡œ ê³ ì •ë˜ëŠ” ê²ƒ(Local Optima)ì„ ë§‰ì•„ì¤ë‹ˆë‹¤.
4. **$c_1, c_2$**: ê° í•­ì˜ ì¤‘ìš”ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ê³„ìˆ˜ì…ë‹ˆë‹¤. (ë³´í†µ $c_1=0.5$, $c_2=0.01$ ì •ë„ ì‚¬ìš©).

---

### ğŸ’¡ PPO Algorithm ìš”ì•½ (Step-by-Step)
ê³ ë“±í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆëŠ” PPOì˜ ì‹¤í–‰ ê³¼ì •ì…ë‹ˆë‹¤.

1. **ë°ì´í„° ìˆ˜ì§‘**: í˜„ì¬ì˜ ì •ì±… $\pi_{\theta_{old}}$ë¡œ ë¡œë´‡(ì—ì´ì „íŠ¸)ì„ ì›€ì§ì—¬ì„œ $T$ì‹œê°„ ë™ì•ˆ ë°ì´í„°ë¥¼ ëª¨ìë‹ˆë‹¤. (ìƒíƒœ, í–‰ë™, ë³´ìƒ ê¸°ë¡).
2. **í‰ê°€**: ëª¨ì€ ë°ì´í„°ë¡œ ê° í–‰ë™ì´ ì–¼ë§ˆë‚˜ ì¢‹ì•˜ëŠ”ì§€($\hat{A}_t$) ê³„ì‚°í•©ë‹ˆë‹¤.
3. **í•™ìŠµ (Epoch ë°˜ë³µ)**:
    - ìˆ˜ì§‘í•œ ë°ì´í„°ë¥¼ ì‘ì€ ë¬¶ìŒ(Mini-batch)ìœ¼ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.
    - ê° ë°°ì¹˜ë§ˆë‹¤ $L^{CLIP}$ ê³µì‹ì„ ì¨ì„œ ê²½ì‚¬ ìƒìŠ¹ë²•(Gradient Ascent)ìœ¼ë¡œ íŒŒë¼ë¯¸í„° $\theta$ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    - ì´ë•Œ, $r_t$ê°€ ë„ˆë¬´ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ë©´ Clipì´ ì‘ë™í•´ì„œ "ì•ˆì „í•˜ê²Œ" ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
4. **ê°±ì‹ **: í•™ìŠµëœ ì •ì±… $\pi_\theta$ë¥¼ ìƒˆë¡œìš´ 'ì˜›ë‚  ì •ì±…' $\pi_{\theta_{old}}$ë¡œ ì‚¼ìŠµë‹ˆë‹¤.
5. **ë°˜ë³µ**: 1ë²ˆìœ¼ë¡œ ëŒì•„ê°€ì„œ ê³„ì† ë°˜ë³µí•©ë‹ˆë‹¤.

### ğŸ”‘ Key Takeaways (í•µì‹¬ ìš”ì•½)
- **PPOëŠ” 'ì•ˆì „í•œ' ê°•í™”í•™ìŠµì´ë‹¤**: ì •ì±…ì´ í•œ ë²ˆì— ë„ˆë¬´ ë§ì´ ë³€í•´ì„œ ë°”ë³´ê°€ ë˜ëŠ” ê²ƒì„ ë§‰ëŠ”ë‹¤.
- **Clippingì´ í•µì‹¬ì´ë‹¤**: ë³µì¡í•œ ìˆ˜í•™ ëŒ€ì‹  `min`ê³¼ `clip`ì´ë¼ëŠ” ë‹¨ìˆœí•œ ì—°ì‚°ìœ¼ë¡œ ì•ˆì •ì„±ì„ í™•ë³´í–ˆë‹¤.
- **ê°€ì„±ë¹„ê°€ ì¢‹ë‹¤**: êµ¬í˜„ì´ ì‰½ê³  ê³„ì‚°ì´ ë¹ ë¥´ë©´ì„œë„ ì„±ëŠ¥ì€ ë§¤ìš° ë›°ì–´ë‚˜ì„œ í˜„ì¬ ê°€ì¥ ë§ì´ ì“°ì´ëŠ” ì•Œê³ ë¦¬ì¦˜ ì¤‘ í•˜ë‚˜ë‹¤.

---

## ğŸ’¡ Key Points
1. **Simplicity**: TRPOë³´ë‹¤ í›¨ì”¬ êµ¬í˜„í•˜ê¸° ì‰½ë‹¤.
2. **Stability**: Clippingì„ í†µí•´ í•™ìŠµì˜ ì•ˆì •ì„±ì„ ë³´ì¥í•œë‹¤.
3. **Sample Efficiency**: ë°ì´í„°ë¥¼ ì¬ì‚¬ìš©(Multiple Epochs)í•  ìˆ˜ ìˆì–´ íš¨ìœ¨ì ì´ë‹¤.

---

## ğŸ“ Quotes
> "Proximal Policy Optimization, a family of policy gradient methods that perform typically as well as or better than TRPO, but are much simpler to implement, more general, and have better sample complexity."

---

## ğŸ”— Related
- [[Trust Region Policy Optimization]] (TRPO)
- [[Policy Gradient]]
- [[Reinforcement Learning]]
